import optparse
import sys
from collections import defaultdict

# IBM model 1
def model_1():

    optparser = optparse.OptionParser()
    optparser.add_option("-d", "--data", dest="train",
                         default="data/hansards", help="Data filename prefix (default=data)")
    optparser.add_option("-e", "--english", dest="english",
                         default="e", help="Suffix of English filename (default=e)")
    optparser.add_option("-f", "--french", dest="french",
                         default="f", help="Suffix of French filename (default=f)")
    optparser.add_option("-n", "--num_sentences", dest="num_sents", default=1000,
                         type="int", help="Number of sentences to use for training")
    optparser.add_option("-i", "--iterations", dest="iterations",
                         default=5, type="int", help="Number of EM iterations")
    (opts, _) = optparser.parse_args()

    f_data = f"{opts.train}.{opts.french}"
    e_data = f"{opts.train}.{opts.english}"
    bitext = [[sentence.strip().split() for sentence in pair]
              for pair in zip(open(f_data), open(e_data))][:opts.num_sents]

    t = defaultdict(lambda: defaultdict(lambda: 1.0))

    for iteration in range(opts.iterations):
        sys.stderr.write(f"Starting iteration {iteration + 1}...\n")

        count_fe = defaultdict(float)
        total_e = defaultdict(float)
        s_total_f = defaultdict(float)

        for (_, (f, e)) in enumerate(bitext):

            for f_i in f:
                s_total_f[f_i] = 0.0
                for e_j in e:
                    s_total_f[f_i] += t[f_i][e_j]

            for f_i in f:
                for e_j in e:
                    count = t[f_i][e_j] / s_total_f[f_i]
                    count_fe[(f_i, e_j)] += count
                    total_e[e_j] += count

        for (f_i, e_j) in count_fe:
            t[f_i][e_j] = count_fe[(f_i, e_j)] / total_e[e_j]

        sys.stderr.write(f"Completed iteration {iteration + 1}\n")

    for (f, e) in bitext:
        for i, f_i in enumerate(f):
            best_j = -1
            best_prob = 0.0
            for j, e_j in enumerate(e):
                if t[f_i][e_j] > best_prob:
                    best_prob = t[f_i][e_j]
                    best_j = j
            sys.stdout.write(f"{i}-{best_j} ")
        sys.stdout.write("\n")


# IBM model 2 (unsupervised)
def model_2():

    optparser = optparse.OptionParser()
    optparser.add_option("-d", "--data", dest="train",
                         default="data/hansards", help="Data filename prefix (default=data)")
    optparser.add_option("-e", "--english", dest="english",
                         default="e", help="Suffix of English filename (default=e)")
    optparser.add_option("-f", "--french", default="f",
                         help="Suffix of French filename (default=f)")
    optparser.add_option("-n", "--num_sentences", default=1000,
                         type="int", help="Number of sentences to use for training")
    optparser.add_option("-i", "--iterations", default=5,
                         type="int", help="Number of EM iterations")
    (opts, _) = optparser.parse_args()

    f_data = f"{opts.train}.{opts.french}"
    e_data = f"{opts.train}.{opts.english}"
    bitext = [[sentence.strip().split() for sentence in pair]
              for pair in zip(open(f_data), open(e_data))][:opts.num_sentences]

    t = defaultdict(lambda: defaultdict(lambda: 1.0))

    a = defaultdict(lambda: defaultdict(
        lambda: defaultdict(lambda: defaultdict(lambda: 1.0))))

    for iteration in range(opts.iterations):
        sys.stderr.write(f"Starting iteration {iteration + 1}...\n")

        count_fe = defaultdict(float)
        total_e = defaultdict(float)
        count_a = defaultdict(lambda: defaultdict(
            lambda: defaultdict(lambda: defaultdict(float))))
        total_a = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))

        for (_, (f, e)) in enumerate(bitext):
            len_f = len(f)
            len_e = len(e)

            for i, f_i in enumerate(f):
                s_total_f = 0.0
                for j, e_j in enumerate(e):
                    s_total_f += t[f_i][e_j] * a[i][j][len_f][len_e]

                for j, e_j in enumerate(e):
                    delta = (t[f_i][e_j] * a[i][j][len_f][len_e]) / s_total_f
                    count_fe[(f_i, e_j)] += delta
                    total_e[e_j] += delta
                    count_a[i][j][len_f][len_e] += delta
                    total_a[j][len_f][len_e] += delta

        for (f_i, e_j) in count_fe:
            t[f_i][e_j] = count_fe[(f_i, e_j)] / total_e[e_j]

        for i in count_a:
            for j in count_a[i]:
                for len_f in count_a[i][j]:
                    for len_e in count_a[i][j][len_f]:
                        a[i][j][len_f][len_e] = count_a[i][j][len_f][len_e] / \
                            total_a[j][len_f][len_e]

        sys.stderr.write(f"Completed iteration {iteration + 1}\n")

    for (f, e) in bitext:
        len_f = len(f)
        len_e = len(e)
        for i, f_i in enumerate(f):
            best_j = -1
            best_prob = 0.0

            for j, e_j in enumerate(e):
                prob = t[f_i][e_j] * a[i][j][len_f][len_e]
                if prob > best_prob:
                    best_prob = prob
                    best_j = j

            sys.stdout.write(f"{i}-{best_j} ")
        sys.stdout.write("\n")


# model_1()
model_2()
