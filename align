import optparse
import sys
from collections import defaultdict
import math

# IBM model 1 function definition
def model_1(): 

  # Parse command line arguments
  optparser = optparse.OptionParser()
  optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
  optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
  optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
  optparser.add_option("-n", "--num_sentences", dest="num_sents", default=1000, type="int", help="Number of sentences to use for training")
  optparser.add_option("-i", "--iterations", dest="iterations", default=5, type="int", help="Number of EM iterations")
  (opts, _) = optparser.parse_args()

  # Load data
  f_data = f"{opts.train}.{opts.french}"
  e_data = f"{opts.train}.{opts.english}"
  bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]

  # Initialize translation probabilities uniformly
  t = defaultdict(lambda: defaultdict(lambda: 1.0))  # t(f|e) initialized to 1.0 for all word pairs

  # Initialize counts
  for iteration in range(opts.iterations):
      sys.stderr.write(f"Starting iteration {iteration + 1}...\n")
      
      # Initialize count and total dictionaries
      count_fe = defaultdict(float)
      total_e = defaultdict(float)
      s_total_f = defaultdict(float)
      
      # E-step: Calculate expected counts
      for (n, (f, e)) in enumerate(bitext):
          # Compute normalization for each French word
          for f_i in f:
              s_total_f[f_i] = 0.0
              for e_j in e:
                  s_total_f[f_i] += t[f_i][e_j]
          
          # Collect counts
          for f_i in f:
              for e_j in e:
                  count = t[f_i][e_j] / s_total_f[f_i]
                  count_fe[(f_i, e_j)] += count
                  total_e[e_j] += count

      # M-step: Recompute translation probabilities
      for (f_i, e_j) in count_fe:
          t[f_i][e_j] = count_fe[(f_i, e_j)] / total_e[e_j]
          
      sys.stderr.write(f"Completed iteration {iteration + 1}\n")

  # Output alignments
  for (f, e) in bitext:
      for i, f_i in enumerate(f):
          best_j = -1
          best_prob = 0.0
          for j, e_j in enumerate(e):
              if t[f_i][e_j] > best_prob:
                  best_prob = t[f_i][e_j]
                  best_j = j
          sys.stdout.write(f"{i}-{best_j} ")
      sys.stdout.write("\n")


# IBM model 2 with Unsupervised Word Alignment with Arbitrary Features function definition
def model_2():

  # Command-line argument parsing
  optparser = optparse.OptionParser()
  optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
  optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
  optparser.add_option("-f", "--french", default="f", help="Suffix of French filename (default=f)")
  optparser.add_option("-n", "--num_sentences", default=1000, type="int", help="Number of sentences to use for training")
  optparser.add_option("-i", "--iterations", default=5, type="int", help="Number of EM iterations")
  (opts, _) = optparser.parse_args()

  # Load data files
  f_data = f"{opts.train}.{opts.french}"
  e_data = f"{opts.train}.{opts.english}"
  bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sentences]

  # Initialize translation probabilities uniformly for all word pairs
  t = defaultdict(lambda: defaultdict(lambda: 1.0))  # Translation probabilities P(f|e)

  # Initialize alignment probabilities uniformly
  a = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 1.0))))  # Alignment probabilities a(i|j, len(f), len(e))

  # EM Iteration Loop
  for iteration in range(opts.iterations):
      sys.stderr.write(f"Starting iteration {iteration + 1}...\n")

      # Initialize counts
      count_fe = defaultdict(float)  # Counts for (f, e)
      total_e = defaultdict(float)   # Total counts for English words
      count_a = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))  # Corrected initialization for count_a
      total_a = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))  # Corrected initialization for total_a

      # E-step: Compute expected counts
      for (n, (f, e)) in enumerate(bitext):
          len_f = len(f)
          len_e = len(e)

          # Compute normalization for each French word
          for i, f_i in enumerate(f):
              s_total_f = 0.0
              for j, e_j in enumerate(e):
                  s_total_f += t[f_i][e_j] * a[i][j][len_f][len_e]

              # Collect counts
              for j, e_j in enumerate(e):
                  delta = (t[f_i][e_j] * a[i][j][len_f][len_e]) / s_total_f
                  count_fe[(f_i, e_j)] += delta
                  total_e[e_j] += delta
                  count_a[i][j][len_f][len_e] += delta
                  total_a[j][len_f][len_e] += delta

      # M-step: Update translation probabilities
      for (f_i, e_j) in count_fe:
          t[f_i][e_j] = count_fe[(f_i, e_j)] / total_e[e_j]

      # M-step: Update alignment probabilities
      for i in count_a:
          for j in count_a[i]:
              for len_f in count_a[i][j]:
                  for len_e in count_a[i][j][len_f]:
                      a[i][j][len_f][len_e] = count_a[i][j][len_f][len_e] / total_a[j][len_f][len_e]

      sys.stderr.write(f"Completed iteration {iteration + 1}\n")

  # Output alignments
  for (f, e) in bitext:
      len_f = len(f)
      len_e = len(e)
      for i, f_i in enumerate(f):
          best_j = -1
          best_prob = 0.0
          # Choose the best alignment j for each French word f_i
          for j, e_j in enumerate(e):
              prob = t[f_i][e_j] * a[i][j][len_f][len_e]
              if prob > best_prob:
                  best_prob = prob
                  best_j = j
          # Output the alignment pair
          sys.stdout.write(f"{i}-{best_j} ")
      sys.stdout.write("\n")

# uncomment whichever model you want to run
model_1()
# model_2()
