import optparse
import sys
from collections import defaultdict
import math

# Command-line argument parsing
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", default=1000, type="int", help="Number of sentences to use for training")
optparser.add_option("-i", "--iterations", default=5, type="int", help="Number of EM iterations")
(opts, _) = optparser.parse_args()

# Load data files
f_data = f"{opts.train}.{opts.french}"
e_data = f"{opts.train}.{opts.english}"
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sentences]

# Initialize translation probabilities uniformly for all word pairs
t = defaultdict(lambda: defaultdict(lambda: 1.0))  # Translation probabilities P(f|e)

# Initialize alignment probabilities uniformly
a = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 1.0))))  # Alignment probabilities a(i|j, len(f), len(e))

# EM Iteration Loop
for iteration in range(opts.iterations):
    sys.stderr.write(f"Starting iteration {iteration + 1}...\n")

    # Initialize counts
    count_fe = defaultdict(float)  # Counts for (f, e)
    total_e = defaultdict(float)   # Total counts for English words
    count_a = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))  # Corrected initialization for count_a
    total_a = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))  # Corrected initialization for total_a

    # E-step: Compute expected counts
    for (n, (f, e)) in enumerate(bitext):
        len_f = len(f)
        len_e = len(e)

        # Compute normalization for each French word
        for i, f_i in enumerate(f):
            s_total_f = 0.0
            for j, e_j in enumerate(e):
                s_total_f += t[f_i][e_j] * a[i][j][len_f][len_e]

            # Collect counts
            for j, e_j in enumerate(e):
                delta = (t[f_i][e_j] * a[i][j][len_f][len_e]) / s_total_f
                count_fe[(f_i, e_j)] += delta
                total_e[e_j] += delta
                count_a[i][j][len_f][len_e] += delta
                total_a[j][len_f][len_e] += delta

    # M-step: Update translation probabilities
    for (f_i, e_j) in count_fe:
        t[f_i][e_j] = count_fe[(f_i, e_j)] / total_e[e_j]

    # M-step: Update alignment probabilities
    for i in count_a:
        for j in count_a[i]:
            for len_f in count_a[i][j]:
                for len_e in count_a[i][j][len_f]:
                    a[i][j][len_f][len_e] = count_a[i][j][len_f][len_e] / total_a[j][len_f][len_e]

    sys.stderr.write(f"Completed iteration {iteration + 1}\n")

# Output alignments
for (f, e) in bitext:
    len_f = len(f)
    len_e = len(e)
    for i, f_i in enumerate(f):
        best_j = -1
        best_prob = 0.0
        # Choose the best alignment j for each French word f_i
        for j, e_j in enumerate(e):
            prob = t[f_i][e_j] * a[i][j][len_f][len_e]
            if prob > best_prob:
                best_prob = prob
                best_j = j
        # Output the alignment pair
        sys.stdout.write(f"{i}-{best_j} ")
    sys.stdout.write("\n")
